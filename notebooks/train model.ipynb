{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural date parser\n",
    "The idea is to generate decodable date strings from natural date strings. We use a seq2seq model that is similar to a denoising autoencoder. Ideally, we want to get from a human written string to a decodable machine-parsable string like so.\n",
    "```\n",
    "STRING_DATE = '01 april 97'\n",
    "TARGET_OUTPUT = '01/04/1997'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd().endswith('notebooks'): os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from decoder_tokenizer import decoder_tokenizer\n",
    "\n",
    "STRING_DATE = '01 april 97'\n",
    "TARGET_OUTPUT = '01/04/1997'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>date_string</th>\n",
       "      <th>decodable_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-12 10:23:07</td>\n",
       "      <td>2020-12-19 12:29:05</td>\n",
       "      <td>12/12/ - 19/12/2020</td>\n",
       "      <td>12/12/2020 - 19/12/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-01-15 10:21:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15 January 97</td>\n",
       "      <td>15/01/1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-05-12 23:32:18</td>\n",
       "      <td>2016-02-09 10:17:53</td>\n",
       "      <td>12 May 12 - 09 February 16</td>\n",
       "      <td>12/05/2012 - 09/02/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1991-04-15 14:51:40</td>\n",
       "      <td>2013-12-10 03:45:10</td>\n",
       "      <td>15 Apr 91 - 10 Dec 13</td>\n",
       "      <td>15/04/1991 - 10/12/2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-10-13 02:04:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 October 13</td>\n",
       "      <td>13/10/2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            start_date             end_date                 date_string  \\\n",
       "0  2020-12-12 10:23:07  2020-12-19 12:29:05         12/12/ - 19/12/2020   \n",
       "1  1997-01-15 10:21:47                  NaN               15 January 97   \n",
       "2  2012-05-12 23:32:18  2016-02-09 10:17:53  12 May 12 - 09 February 16   \n",
       "3  1991-04-15 14:51:40  2013-12-10 03:45:10       15 Apr 91 - 10 Dec 13   \n",
       "4  2013-10-13 02:04:23                  NaN               13 October 13   \n",
       "\n",
       "          decodable_string  \n",
       "0  12/12/2020 - 19/12/2020  \n",
       "1               15/01/1997  \n",
       "2  12/05/2012 - 09/02/2016  \n",
       "3  15/04/1991 - 10/12/2013  \n",
       "4               13/10/2013  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2, 55,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('synthetic_datestrings.csv', index_col = 0)\n",
    "display( df.head() )\n",
    "\n",
    "tokenizer = decoder_tokenizer()\n",
    "tokenizer.fit(df.decodable_string)\n",
    "tokenized_target = tokenizer.tokenize([STRING_DATE])\n",
    "tokenized_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, string\n",
    "\n",
    "def apply_noisy_insertion(noiseless_string, noise):\n",
    "    assert noise < 1\n",
    "    if random.uniform(0,1) < noise:\n",
    "        inserted_letter = random.choice(string.ascii_letters)\n",
    "        insertion_position = np.random.randint(len(noiseless_string))\n",
    "        return noiseless_string[:insertion_position] + inserted_letter + noiseless_string[insertion_position:]\n",
    "    else:\n",
    "        return noiseless_string\n",
    "    \n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Generates data for Keras\n",
    "    https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer=tokenizer, noise=0.4,\n",
    "                 batch_size=32, shuffle=True):\n",
    "        self.df = df\n",
    "        self.natural_string_input = df.date_string\n",
    "        self.output_string = df.decodable_string\n",
    "        self.indexes = df.index.values\n",
    "        self.batch_size = batch_size\n",
    "        self.noise = noise\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_classes=tokenizer.vocab_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.indexes))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # PREPARE TRAINING DATA\n",
    "        # natural string dates\n",
    "        natural_date_str_batch = df.loc[indexes].date_string.copy()\n",
    "        if self.noise>0: natural_date_str_batch = natural_date_str_batch.apply(lambda x: apply_noisy_insertion(x, self.noise))\n",
    "        natural_date_str_batch = natural_date_str_batch.values\n",
    "        \n",
    "        # decodable strings, these are the desired output\n",
    "        decodable_string_batch = df.loc[indexes].decodable_string\n",
    "\n",
    "        # teacher forcing inputs\n",
    "        decoder_inputs_array = self.tokenizer.tokenize(decodable_string_batch)\n",
    "        decoder_inputs_array = to_categorical(\n",
    "            decoder_inputs_array, num_classes=self.n_classes, dtype='float32'\n",
    "        )\n",
    "\n",
    "        # true labels, lags teacher forcing inputs by 1 time step\n",
    "        decoder_outputs_array = self.tokenizer.tokenize(decodable_string_batch, sos=False)\n",
    "        decoder_outputs_array = to_categorical(\n",
    "            decoder_outputs_array, num_classes=self.n_classes, dtype='float32'\n",
    "        )\n",
    "        \n",
    "        return [natural_date_str_batch, decoder_inputs_array], decoder_outputs_array\n",
    "        \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "from decoder_tokenizer import decoder_tokenizer\n",
    "\n",
    "class natural_datetime_use:\n",
    "    \n",
    "    def define_train_model(self, use_path = \"resources/muse\"):\n",
    "        self.embed = hub.KerasLayer(\"resources/muse\", \n",
    "                                    input_shape=[],     # Expects a tensor of shape [batch_size] as input.\n",
    "                                    trainable=True\n",
    "                                    )\n",
    "        \n",
    "        self.LATENT_DIM = self.embed(['01 Apr 97']).shape[1]\n",
    "        \n",
    "        # https://github.com/tensorflow/hub/issues/648\n",
    "        natural_date_str = keras.Input(shape=[], dtype=tf.string)\n",
    "        encoding = self.embed(natural_date_str)\n",
    "\n",
    "        # Set up the decoder, using `encoder_states` as initial state.\n",
    "        decoder_inputs = keras.Input(shape=(None, self.tokenizer.vocab_size))\n",
    "        decoder_lstm = keras.layers.LSTM(self.LATENT_DIM, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[encoding, encoding])\n",
    "        decoder_dense = keras.layers.Dense(self.tokenizer.vocab_size, activation=\"softmax\")\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        # Define the model that will turn\n",
    "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "        self.training_model = keras.Model([natural_date_str, decoder_inputs], decoder_outputs)\n",
    "        self.training_model.compile(\n",
    "            optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def define_decoder_model(self):\n",
    "        # define resulting decoder\n",
    "        decoder_inputs = self.training_model.input[1]  # input_2\n",
    "        decoder_state_input_h = keras.Input(shape=(self.LATENT_DIM,), name=\"hiddenstate_input\")\n",
    "        decoder_state_input_c = keras.Input(shape=(self.LATENT_DIM,), name=\"cellstate_input\")\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_lstm = self.training_model.layers[3]\n",
    "        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "            decoder_inputs, initial_state=decoder_states_inputs\n",
    "        )\n",
    "        decoder_states = [state_h_dec, state_c_dec]\n",
    "        decoder_dense = self.training_model.layers[4]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = keras.Model(\n",
    "            [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def train_on_synthetic_data(self, \n",
    "                                data_path = 'synthetic_datestrings.csv', \n",
    "                                batch_size = 64,\n",
    "                                epochs=5\n",
    "                               ):\n",
    "        \n",
    "        # train tokenizer\n",
    "        df = pd.read_csv('synthetic_datestrings.csv', index_col = 0)\n",
    "        self.tokenizer = decoder_tokenizer()\n",
    "        self.tokenizer.fit(df.decodable_string)\n",
    "        \n",
    "        # define model\n",
    "        self.define_train_model()\n",
    "        \n",
    "        # PREPARE TRAINING DATA\n",
    "        training_generator = DataGenerator(df.iloc[:int(len(df)*0.8)], \n",
    "                                           tokenizer=self.tokenizer, \n",
    "                                           batch_size=batch_size)\n",
    "        valid_generator = DataGenerator(df.iloc[int(len(df)*0.8):], \n",
    "                                        tokenizer=self.tokenizer, \n",
    "                                        batch_size=batch_size,\n",
    "                                        noise=0)\n",
    "\n",
    "        # train model\n",
    "        self.training_model.fit_generator(\n",
    "            generator=training_generator,\n",
    "            validation_data=valid_generator,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "        \n",
    "        self.define_decoder_model()\n",
    "\n",
    "    def predict(self, natural_date_str):\n",
    "        # get encodings\n",
    "        encoding = self.embed([natural_date_str])\n",
    "\n",
    "        # initialize decoder output\n",
    "        decoder_input = np.zeros((1,1,self.tokenizer.vocab_size))\n",
    "        sos_id = self.tokenizer.t.texts_to_sequences(['sos'])[0][0]\n",
    "        eos_id = self.tokenizer.t.texts_to_sequences(['eos'])[0][0]\n",
    "        decoder_input[:,:,sos_id]=1\n",
    "\n",
    "        hidden_state = cell_state = encoding\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        predicted_tokens, stop_condition = [], False\n",
    "        while not stop_condition:\n",
    "\n",
    "            # passing enc_output to the decoder\n",
    "            # and get loss\n",
    "            decoder_prediction, hidden_state, cell_state = self.decoder_model([decoder_input, \n",
    "                                                                              hidden_state, \n",
    "                                                                              cell_state]) \n",
    "            token_id = decoder_prediction.numpy().argmax()\n",
    "            predicted_tokens.append(token_id)\n",
    "\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop character.\n",
    "            if token_id == eos_id or len(predicted_tokens) > self.tokenizer.MAX_LEN:\n",
    "                stop_condition = True\n",
    "\n",
    "            # apply teacher forcing\n",
    "            # prepare next (correct) token\n",
    "            decoder_input = np.zeros((1,1,self.tokenizer.vocab_size))\n",
    "            decoder_input[:,:,token_id]=1\n",
    "        \n",
    "        detokenized = self.tokenizer.detokenize([predicted_tokens])\n",
    "        start , end = parse_decodable_str(detokenized)\n",
    "        return start , end \n",
    "\n",
    "    def save(self, path=\"savedmodel\"):\n",
    "        self.tokenizer.save(f\"{path}/tokenizer.json\")\n",
    "        tf.saved_model.save( keras.Sequential([keras.Input(shape=[], dtype=tf.string), self.embed]) , \n",
    "                            f\"{path}/muse\")\n",
    "        self.decoder_model.save(f\"{path}/decoder_model\")\n",
    "        \n",
    "    def load(self, path=\"savedmodel\"):\n",
    "        self.tokenizer.load(f\"{path}/tokenizer.json\")\n",
    "        self.embed = hub.KerasLayer(f\"{path}/muse\", \n",
    "                            input_shape=[],     # Expects a tensor of shape [batch_size] as input.\n",
    "                            trainable=False\n",
    "                            )\n",
    "        self.decoder_model = tf.keras.models.load_model(f\"{path}/decoder_model\")\n",
    "        \n",
    "\n",
    "def parse_decodable_str(decodable_str):\n",
    "    \"\"\"\n",
    "    start, end = parse_decodable_str('26 / 01 / 2012 - 27 / 06 / 2016 eos')\n",
    "    start, end\n",
    "    >>> (datetime.datetime(2012, 1, 26, 0, 0), datetime.datetime(2012, 1, 26, 0, 0))\n",
    "    \"\"\"\n",
    "    decodable_str = decodable_str.replace(' eos', '')\n",
    "    \n",
    "    split_decodable_str = decodable_str.split(' - ')\n",
    "    if len(split_decodable_str) == 1:\n",
    "        return datetime.datetime.strptime('26 / 01 / 2012', '%d / %m / %Y'), None\n",
    "    elif len(split_decodable_str) == 2:\n",
    "        return parse_decodable_str(split_decodable_str[0])[0], parse_decodable_str(split_decodable_str[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinitrinh/anaconda3/envs/gr2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 442s 333ms/step - loss: 0.8846 - accuracy: 0.7807 - val_loss: 0.4671 - val_accuracy: 0.8343\n",
      "Epoch 2/10\n",
      " 582/1250 [============>.................] - ETA: 3:31 - loss: 0.4568 - accuracy: 0.8382"
     ]
    }
   ],
   "source": [
    "parser = natural_datetime_use()\n",
    "parser.train_on_synthetic_data(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.predict(\"1 May 2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.save()\n",
    "parser.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.predict(\"1 May 2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
